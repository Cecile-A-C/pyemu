{
 "metadata": {
  "name": "",
  "signature": "sha256:88394f01f623601769e942347d3f1ddc36ff6cd93bbc21e57e46277837690111"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Model background\n",
      "Here is an example based on the henry saltwater intrusion problem.  The synthetic model is a 2-dimensional SEAWAT model (X-Z domain) with 1 row, 120 columns and 20 layers.  The left boundary is a specified flux of freshwater, the right boundary is a specified head and concentration saltwater boundary.  The model has two stress periods: an initial steady state (calibration) period, then a transient period with less flux (prediction).  \n",
      "\n",
      "![henry domain](domain.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The inverse problem includes 601 parameters (600 hydraulic conductivity pilot points and 1 specified flux rate) and 36 obseravtions (21 heads and 15 concentrations) measured at the end of the steady state calibration period.  Predictions of head and concentration are also available at each observation location at the end of the transient prediction period.  The predictions we will focus on predicting concentrations at location 13, 10, and 05 at the end of the transient stress period.  I previously calculated the jacobian matrix, which is in the `henry/` folder, along with the pest control file.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Using `pyemu`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "import pyemu\n",
      "reload(pyemu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First create a linear_analysis object.  We will use `err_var`  derived type, which replicates the behavior of the `PREDVAR` suite of PEST as well as `ident_par` utility.  We pass it the name of the jacobian matrix file.  Since we don't pass an explicit argument for `parcov` or `obscov`, `pyemu` attempts to build them from the parameter bounds and observation weights in a pest control file (.pst) with the same base case name as the jacobian.  Since we are interested in predictive uncertainty as well as parameter uncertainty, we also pass the names of the prediction sensitivity vectors we are interested in, which are stored in the jacobian as well.  Note that the `predictions` argument can be a mixed list of observation names, other jacobian files or PEST-compatible ASCII matrix files.  Remember you can pass a filename to the `verbose` argument to write log file.\n",
      "\n",
      "Since most groundwater model history-matching analyses focus on adjusting hetergeneous hydraulic properties and not boundary condition elements, let's identify the `mult1` parameter as `omitted` in the error variance analysis.  We can conceptually think of this action as excluding the `mult1` parameter from the history-matching process. Later we will explicitly calculate the penalty for not adjusting this parameter"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictions = [\"C_obs13_2\",\"c_obs10_2\",\"c_obs05_2\"]\n",
      "la = pyemu.errvar(jco=os.path.join(\"henry\", \"pest.jco\"), predictions=predictions, omitted_parameters=\"mult1\", verbose=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "for this particular analysis, we want to drop the regularization information from the jacobian:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "la.drop_prior_information()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Parameter identifiability\n",
      "The `errvar` dervied type exposes a method to get a `pandas` dataframe of parameter identifiability information.  Recall that parameter identifiability is expressed as $d_i = \\Sigma(\\mathbf{V}_{1i})^2$, where $d_i$ is the parameter identifiability, which ranges from 0 (not identified by the data) to 1 (full identified by the data), and $\\mathbf{V}_1$ are the right singular vectors corresonding to non-(numerically) zero singular values.  First let's look at the singular spectrum of $\\mathbf{Q}^{\\frac{1}{2}}\\mathbf{J}$, where $\\mathbf{Q}$ is the cofactor matrix and $\\mathbf{J}$ is the jacobian:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = la.qhalfx.s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as plt\n",
      "figure = plt.figure(figsize=(10, 5))\n",
      "ax = plt.subplot(111)\n",
      "ax.plot(s.x)\n",
      "ax.set_title(\"singular spectrum\")\n",
      "ax.set_ylabel(\"power\")\n",
      "ax.set_xlabel(\"singular value\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the singluar spectrum decays rapidly (not uncommon) and that we can really only support about 15 right singular vectors.  Let's get the identifiability dataframe at 15 singular vectors:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ident_df = la.get_identifiability_dataframe(15) # the method is passed the number of singular vectors to include in V_1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot the indentifiability:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print ident_df[\"right_sing_vec_1\"].values\n",
      "idx = np.arange(len(la.jco.par_names))\n",
      "width = 0.4\n",
      "fig = plt.figure(figsize=(20, 10))\n",
      "ax = plt.subplot(111)\n",
      "ax.bar(idx[:500],ident_df[\"ident\"].values[:500],width=width,facecolor='g',edgecolor=\"none\")\n",
      "#for ising in xrange(5):\n",
      "ax.set_xlim(-1,602)    \n",
      "plt.show()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As expected in a highly parameterized inversion, we have \"spread out\" the identifiability to many individual hydraulic conductivity parameters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Predictive error variance \n",
      "\n",
      "Now let's explore the error variance of the predictions we are interested in.  We will use an extended version of the predictive error variance equation:   \n",
      "\n",
      "$\\sigma_{s - \\hat{s}}^2 = \\underbrace{\\textbf{y}_i^T({\\bf{I}} - {\\textbf{R}})\\boldsymbol{\\Sigma}_{{\\boldsymbol{\\theta}}_i}({\\textbf{I}} - {\\textbf{R}})^T\\textbf{y}_i}_{1} + \\underbrace{{\\textbf{y}}_i^T{\\bf{G}}\\boldsymbol{\\Sigma}_{\\mathbf{\\epsilon}}{\\textbf{G}}^T{\\textbf{y}}_i}_{2} + \\underbrace{{\\bf{p}}\\boldsymbol{\\Sigma}_{{\\boldsymbol{\\theta}}_o}{\\bf{p}}^T}_{3}$\n",
      "\n",
      "Where term 1 is the null-space contribution, term 2 is the solution space contribution and term 3 is the model error term (the penalty for not adjusting uncertain parameters).  Remember the `mult1` parameter that we marked as omitted?  The consequences of that action can now be explicitly evaluated.  See Moore and Doherty (2005) and White and other (2014) for more explanation of these terms.  Note that if you don't have any `omitted_parameters`, the only terms 1 and 2 contribute to error variance\n",
      "\n",
      "First we need to create a list (or numpy ndarray) of the singular values we want to test.  Since we have $\\lt40$ data, we only need to test up to $40$ singular values because that is where the action is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sing_vals = np.arange(40)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `errvar` derived type exposes a convience method to get a multi-index pandas dataframe with each of the terms of the error variance equation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "errvar_df = la.get_errvar_dataframe(sing_vals)\n",
      "print errvar_df.index\n",
      "print errvar_df.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "plot the error variance components for each prediction:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(20, 20))\n",
      "ax_1, ax_2, ax_3 = plt.subplot(311), plt.subplot(312), plt.subplot(313)\n",
      "axes = [ax_1,ax_2,ax_3]\n",
      "\n",
      "colors = {\"first\": 'g', \"second\": 'b', \"third\": 'c'}\n",
      "max_idx = 19\n",
      "idx = sing_vals[:max_idx]\n",
      "for ipred, pred in enumerate(predictions):\n",
      "    pred = pred.lower()\n",
      "    ax = axes[ipred]\n",
      "    ax.set_title(pred)\n",
      "    first = errvar_df[(\"first\", pred)][:max_idx]\n",
      "    second = errvar_df[(\"second\", pred)][:max_idx]\n",
      "    third = errvar_df[(\"third\", pred)][:max_idx]\n",
      "    ax.bar(idx, first, edgecolor=\"none\", facecolor=colors[\"first\"], label=\"first\",bottom=0.0)\n",
      "    ax.bar(idx, second, edgecolor=\"none\", facecolor=colors[\"second\"], label=\"second\", bottom=first)\n",
      "    ax.bar(idx, third, edgecolor=\"none\", facecolor=colors[\"third\"], label=\"third\", bottom=second+first)\n",
      "    ax.set_xlim(-1,max_idx+1)\n",
      "    ax.set_xticks(idx+0.5)\n",
      "    ax.set_xticklabels(idx)\n",
      "    ax.set_xlabel(\"singular value\")\n",
      "    ax.set_ylabel(\"error variance\")\n",
      "    ax.legend(loc=\"upper left\")\n",
      "    \n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we see the trade off between getting a good fit to push down the null-space (1st) term and the penalty for overfitting (the rise of the solution space (2nd) term)).  We also see the added penalty for not adjusting the `mult1` parameter (3rd term).  Notice how the shape of the trade off is different for each of the three predictions, implying that we should calibrate the model differently for each prediction.  Note that the error variance at 0 singular values is the prior uncertainty of the prediction."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}