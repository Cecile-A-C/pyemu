{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freyberg Model Schur Complement Example\n",
    "This example uses a synthetic model (described below) to illustrate the `pyemu` capabilities of the Schur complement for calculating posterior covariance.\n",
    "\n",
    "Note that, in addition to `pyemu`, this notebook relies on `flopy`. `flopy` can be obtained (along with installation instructions) at https://github.com/modflowpy/flopy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle as rect\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model background\n",
    "This example is based on the synthetic classroom model of Freyberg(1988).  The  model is a 2-dimensional MODFLOW model with 1 layer,  40 rows, and 20 columns.  The model has 2 stress periods: an initial steady-state stress period used for calibration, and a 5-year transient stress period.  The calibration period uses the recharge and well flux of Freyberg(1988); the last stress period use 25% less recharge and 25% more pumping to represent future conditions for a forecast period.\n",
    "\n",
    "The inverse problem has 761 parameters: hydraulic conductivity of each active model cell, calibration and forecast period recharge multipliers, storage and specific yield, calibration and forecast well flux for each of the six wells, and river bed conductance for each 40 cells with river-type boundary conditions.  The inverse problem has 13 head obseravtions, measured at the end of the steady-state calibration period.  The forecasts of interest include the sw-gw exchange flux during both stress periods (observations named ```sw_gw_0``` and ``sw_gw_1``), and the water level in well cell 6 located in at row 28 column 5 at the end of the stress periods (observations named ```or28c05_0``` and ```or28c05_1```).  The forecasts are included in the Jacobian matrix as zero-weight observations. The model files, pest control file and previously-calculated jacobian matrix are in the `freyberg/` folder\n",
    "\n",
    "\n",
    "Freyberg, David L. \"AN EXERCISE IN GROUND‚ÄêWATER MODEL CALIBRATION AND PREDICTION.\" Groundwater 26.3 (1988): 350-360."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import flopy\n",
    "\n",
    "# load the model\n",
    "model_ws = os.path.join(\"Freyberg\",\"extra_crispy\")\n",
    "ml = flopy.modflow.Modflow.load(\"freyberg\",model_ws=model_ws)\n",
    "\n",
    "# plot some model attributes\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = plt.subplot(111,aspect=\"equal\")\n",
    "ml.upw.hk.plot(axes=[ax],colorbar=\"K m/d\",alpha=0.3)\n",
    "ml.wel.stress_period_data.plot(axes=[ax])\n",
    "ml.riv.stress_period_data.plot(axes=[ax])\n",
    "\n",
    "# plot obs locations\n",
    "obs = pd.read_csv(os.path.join(\"freyberg\",\"misc\",\"obs_rowcol.dat\"),delim_whitespace=True)\n",
    "obs_x = [ml.dis.sr.xcentergrid[r-1,c-1] for r,c in obs.loc[:,[\"row\",\"col\"]].values]\n",
    "obs_y = [ml.dis.sr.ycentergrid[r-1,c-1] for r,c in obs.loc[:,[\"row\",\"col\"]].values]\n",
    "ax.scatter(obs_x,obs_y,marker='.',label=\"obs\")\n",
    "\n",
    "#plot names on the pumping well locations\n",
    "wel_data = ml.wel.stress_period_data[0]\n",
    "wel_x = ml.dis.sr.xcentergrid[wel_data[\"i\"],wel_data[\"j\"]]\n",
    "wel_y = ml.dis.sr.ycentergrid[wel_data[\"i\"],wel_data[\"j\"]]\n",
    "for i,(x,y) in enumerate(zip(wel_x,wel_y)):\n",
    "    ax.text(x,y,\"{0}\".format(i+1),ha=\"center\",va=\"center\")\n",
    "\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_xlabel(\"x\")\n",
    "\n",
    "ax.add_patch(rect((0,0),0,0,label=\"well\",ec=\"none\",fc=\"r\"))\n",
    "ax.add_patch(rect((0,0),0,0,label=\"river\",ec=\"none\",fc=\"g\"))\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.5,1.0),frameon=False)\n",
    "plt.savefig(\"domain.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the Freyberg (1988) model domain.  The colorflood is the hydraulic conductivity ($\\frac{m}{d}$).  Red and green cells coorespond to well-type and river-type boundary conditions. Blue dots the locations of water levels used for calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `pyemu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyemu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a linear_analysis object.  We will use the `schur`  derived type, which replicates the behavior of the `PREDUNC` suite of PEST.  We pass it the name of the Jacobian matrix file.  Since we don't pass an explicit argument for `parcov` or `obscov`, `pyemu` attempts to build them from the parameter bounds and observation weights in a pest control file (.pst) with the same base case name as the Jacobian.  This assumes that the bounds represent the mean value + and - 2 times the standard deviation.\n",
    "\n",
    "Since we are interested in forecast uncertainty as well as parameter uncertainty, we also pass the names of the forecast sensitivity vectors we are interested in, which are stored in the Jacobian as well.  Note that the `forecasts` argument can be a mixed list of observation names, other Jacobian files or PEST-compatible ASCII matrix files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the list of forecast names from the pest++ argument in the pest control file\n",
    "jco = os.path.join(\"freyberg\",\"freyberg.jcb\") # just set the path and filename for the jco file\n",
    "pst = pyemu.Pst(jco.replace(\"jcb\",\"pst\"))     # use the jco name with extention \"pst\" for the control file\n",
    "la = pyemu.Schur(jco=jco, forecasts=pst.pestpp_options[\"forecasts\"].split(','),verbose=False)\n",
    "print(\"observations,parameters in Jacobian:\",la.jco.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Parameter uncertainty analysis\n",
    "Let's calculate and save the posterior parameter covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "la.posterior_parameter.to_ascii(jco+\"_post.cov\") #writes posterior covariance to a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open this file in a text editor to examine.  The diagonal of this matrix is the posterior variance of each parameter. Since we already calculated the posterior parameter covariance matrix, additional calls to the `posterior_parameter` decorated methods only require access--they do not recalculate the matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "la.posterior_parameter.to_dataframe().sort().sort(axis=1).iloc[0:3,0:3] #look so nice in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the posterior variance of each parameter along the diagonal of this matrix. Now, let's make a simple plot of prior vs posterior uncertainty for the 761 parameters. The ``.get_parameters_summary()`` method is the easy way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the parameter uncertainty dataframe and sort it\n",
    "par_sum = la.get_parameter_summary().sort_values(\"percent_reduction\",ascending=False)\n",
    "#make a bar plot of the percent reduction ((prior - posterior) / prior) for the first 20 parameters \n",
    "par_sum.loc[par_sum.index[:20],\"percent_reduction\"].plot(kind=\"bar\",figsize=(10,4),edgecolor=\"none\")\n",
    "#echo the first 10 entries\n",
    "par_sum.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can plot up the top 10 percent reductions\n",
    "par_sum.iloc[0:10,:]['percent_reduction'].plot(kind='bar')\n",
    "plt.title('Percent Reduction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can plot up the prior and posterior variance of the top 10 percent reductions\n",
    "par_sum.iloc[0:10,:][['prior_var','post_var']].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that calibrating the model to the 13 water levels reduces the uncertainty of the calibration period recharge parameter (`rch_0`) by 50%.  Additionally, the hydraulic conductivity of many model cell is also reduced.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the other end of the parameter uncertainty summary -- the values with the _least_ amount of uncertainty reduction.  Note that calling ``get_parameter_summary()`` again results in no additional computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort in increasing order\n",
    "par_sum = la.get_parameter_summary().sort_values(\"percent_reduction\")\n",
    "# plot the first 20\n",
    "par_sum.loc[par_sum.index[:20],\"percent_reduction\"].plot(kind=\"bar\",figsize=(10,4),edgecolor=\"none\")\n",
    "#echo the first 10 \n",
    "par_sum.iloc[0:20,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that several parameters are uneffected by calibration - these are mostly parameters that represent forecast period uncertainty (parameters that end with ```_2```), but there are also some hydraulic conductivities that are uninformed by the 13 water level observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a map of uncertainty reduction for the hydraulic conductivity parameters using some ```flopy``` action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hk_pars = par_sum.loc[par_sum.groupby(lambda x:\"hk\" in x).groups[True],:]\n",
    "hk_pars.loc[:,\"names\"] = hk_pars.index\n",
    "names = hk_pars.names\n",
    "hk_pars.loc[:,\"i\"] = names.apply(lambda x: int(x[3:5]))\n",
    "hk_pars.loc[:,\"j\"] = names.apply(lambda x: int(x[6:8]))\n",
    "unc_array = np.zeros_like(ml.upw.hk[0].array) - 1\n",
    "for i,j,unc in zip(hk_pars.i,hk_pars.j,hk_pars.percent_reduction):\n",
    "    unc_array[i,j] = unc \n",
    "unc_array[unc_array == -1] = np.NaN\n",
    "\n",
    "# plot some model attributes\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.subplot(111,aspect=\"equal\")\n",
    "cb = ax.imshow(unc_array,interpolation=\"none\",alpha=0.5,extent=ml.dis.sr.get_extent())\n",
    "plt.colorbar(cb,label=\"percent uncertainty reduction\")\n",
    "#ml.wel.stress_period_data.plot(axes=[ax])\n",
    "ml.riv.stress_period_data.plot(axes=[ax])\n",
    "\n",
    "# plot obs locations\n",
    "obs = pd.read_csv(os.path.join(\"freyberg\",\"misc\",\"obs_rowcol.dat\"),delim_whitespace=True)\n",
    "obs_x = [ml.dis.sr.xcentergrid[r-1,c-1] for r,c in obs.loc[:,[\"row\",\"col\"]].values]\n",
    "obs_y = [ml.dis.sr.ycentergrid[r-1,c-1] for r,c in obs.loc[:,[\"row\",\"col\"]].values]\n",
    "ax.scatter(obs_x,obs_y,marker='.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, most of the information in the observations is flowing to the hydraulic conductivity parameters near observations themselves. Areas farther from the observations experience less reduction in uncertainty due to calibration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast uncertainty\n",
    "Now let's examine the prior and posterior variance of the forecasts. The uncertainty in parameters directly impacts the uncertainty of forecasts made with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the forecast summary then make a bar chart of the percent_reduction column\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = plt.subplot(111)\n",
    "ax = la.get_forecast_summary().percent_reduction.plot(kind='bar',ax=ax,grid=True)\n",
    "ax.set_ylabel(\"percent uncertainy\\nreduction from calibration\")\n",
    "ax.set_xlabel(\"forecast\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"forecast_sum.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the spread on the uncertainty reduction: some forecasts benefit more from calibration than others.  For example, ```or28c05_0```, the calibration-period water level forecast, benefits from calibration since its uncertainty is reduced by 75%, while ```sw_gw_1```, the forecast-period surface-water groundwater exchange forecast does not benefit from calibration - its uncertainty is unchanged by calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data worth--evaluating the value of existing observations\n",
    "Now, let's try to identify which observations are most important to reducing the posterior uncertainty (e.g.the forecast worth of every observation).  We simply recalculate Schur's complement without some observations and see how the posterior forecast uncertainty increases\n",
    "\n",
    "```get_importance_dataframe_groups()``` is a thin wrapper that calls the underlying ```get_importance_dataframe()``` method using the observation groups in the pest control file and stacks the results into a ```pandas DataFrame```.  \n",
    "\n",
    "This call will test all of the non-forecast observations in the PEST data set to see which ones are most important \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "la.get_importance_dataframe_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's determine which observations are calibration observations. These are the ones with nonzero weight, accessed from the `pst` object as the `nnz_obs_names` property, which returns a list. We can make it into a dictionary and see that, as expected, it has 13 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obs_dict = dict(zip(pst.nnz_obs_names,pst.nnz_obs_names))\n",
    "print(len(obs_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we can pass that dictionary to the `la.get_importance_dataframe` method to return the importance of those 13 observations into a dataframe we'll call `df_worth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_worth= la.get_importance_dataframe(obs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_worth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```base``` row contains the results of the Schur's complement calculation using all observations.  The increase in posterior forecast uncertainty for each of the 13 water level observations (e.g. or17c17 is the observation in row 18 column 18) show how much forecast uncertainty increases when that particular observation is not used in history matching.  So we see again that each forecast depends on the observations differently.\n",
    "\n",
    "We can normalize the importance to the maximum importance value to create a metric of data worth which will be between 0 and 100%. Then we can also determine which observation has the highest data worth with respect to each forecast and also report how much reduction in uncertainty it is responsible for (e.g. how much does forecast uncertainty increase if that data point is not used for history matching)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a little processing of df_worth\n",
    "df_base = df_worth.loc[\"base\",:].copy()\n",
    "df_imax = df_worth.apply(lambda x:x-df_base,axis=1).idxmax()\n",
    "df_max = 100.0 * (df_worth.apply(lambda x:x-df_base,axis=1).max() / df_base)\n",
    "df_par = pd.DataFrame([df_imax,df_max],index=[\"most important observation\",\"percent increase when left out\"])\n",
    "df_par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that observation ```or27c07_0``` is the most important for the water level forecasts (```or28c05_0``` and ```or28c05_1```), while observation ```or10c02_0``` is the most important for the surface water groundwater exchange forecasts (```sw_gw_0``` and ```sw_gw_1```). Also, observation ```or10c02_0```) results ina much greater increase in uncertainty for forecast ```sw_gw_0``` than it does for ```sw_gw_1```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data worth--evaluating the potential value of new observations\n",
    "A potential water-level observation for each active model cell was also \"carried\" in the PEST control file.  This means we can run this same analysis to find the best next place to collect a new water level.  This takes a little longer because it is rerunning the schur's complement calculations many times, so this section can be skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the potential observation locations\n",
    "First we need a list of the observations with zero weight and that start with `\"or\"`--- (these are the synthetic proposed locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_obs_list = [i[1].obsnme for i in pst.observation_data.iterrows() \n",
    "                if i[1].weight==0 and i[1].obsnme[0:2] =='or' ]\n",
    "print (len(pst.observation_data))\n",
    "print (len(new_obs_list))\n",
    "new_obs_dict = dict(zip(new_obs_list,new_obs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_worth_new= la.get_importance_dataframe(new_obs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_worth_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hk_pars = par_sum.loc[par_sum.groupby(lambda x:\"hk\" in x).groups[True],:]\n",
    "hk_pars.loc[:,\"names\"] = hk_pars.index\n",
    "names = hk_pars.names\n",
    "hk_pars.loc[:,\"i\"] = names.apply(lambda x: int(x[3:5]))\n",
    "hk_pars.loc[:,\"j\"] = names.apply(lambda x: int(x[6:8]))\n",
    "unc_array = np.zeros_like(ml.upw.hk[0].array) - 1\n",
    "for i,j,unc in zip(hk_pars.i,hk_pars.j,hk_pars.percent_reduction):\n",
    "    unc_array[i,j] = unc \n",
    "unc_array[unc_array == -1] = np.NaN\n",
    "\n",
    "# plot some model attributes\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.subplot(111,aspect=\"equal\")\n",
    "cb = ax.imshow(unc_array,interpolation=\"none\",alpha=0.5,extent=ml.dis.sr.get_extent())\n",
    "plt.colorbar(cb,label=\"percent uncertainty reduction\")\n",
    "#ml.wel.stress_period_data.plot(axes=[ax])\n",
    "ml.riv.stress_period_data.plot(axes=[ax])\n",
    "\n",
    "# plot obs locations\n",
    "obs = pd.read_csv(os.path.join(\"freyberg\",\"misc\",\"obs_rowcol.dat\"),delim_whitespace=True)\n",
    "obs_x = [ml.dis.sr.xcentergrid[r-1,c-1] for r,c in obs.loc[:,[\"row\",\"col\"]].values]\n",
    "obs_y = [ml.dis.sr.ycentergrid[r-1,c-1] for r,c in obs.loc[:,[\"row\",\"col\"]].values]\n",
    "ax.scatter(obs_x,obs_y,marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parameter contribution to uncertainty\n",
    "Lets look at which parameters are contributing most to forecast uncertainty.  for demostration purposes, lets group the parameters by parameter group name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = la.get_contribution_dataframe_groups()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select out the posterior column\n",
    "df_reduce = df.xs(\"post\",axis=1,level=1)\n",
    "#calc the percent reduction in posterior\n",
    "df_percent = 100.0 * (df_reduce.loc[\"base\",:]-df_reduce)/df_reduce.loc[\"base\",:]\n",
    "#drop the base column\n",
    "df_percent = df_percent.iloc[1:,:]\n",
    "#transpose and plot\n",
    "df_percent.T.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some interesting results here.  The sw-gw flux during calibration (sw_gw_0) is influenced by both recharge and hk uncertainty, but the forecast period sw-gw flux is influenced most by forecast period recharge uncertainty.  unfortunately, model calibration does not tell us about forecast period forcing, so calibration doesn't help with this forecast.  For the water level forecasts (```or28c05_0 and or28c05_1```), the results are similar: the forecast of water level at the end of the calibration period benefits most from hk knowledge, while the forecast period water level is most by recharge and storage.  Let's do this same analysis, but now group the parameters differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pnames = la.pst.par_names\n",
    "fore_names = [pname for pname in pnames if pname.endswith(\"_2\")]\n",
    "props = [pname for pname in pnames if pname[:2] in [\"hk\",\"ss\",\"sy\",\"rc\"] and \"rch\" not in pname]\n",
    "cal_names = [pname for pname in pnames if pname.endswith(\"_1\")]\n",
    "pdict = {'forecast forcing':fore_names,\"properties\":props,\"calibration forcing\":cal_names}\n",
    "df = la.get_contribution_dataframe(pdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select out the posterior column\n",
    "df_reduce = df.xs(\"post\",axis=1,level=1)\n",
    "#calc the percent reduction in posterior\n",
    "df_percent = 100.0 * (df_reduce.loc[\"base\",:]-df_reduce)/df_reduce.loc[\"base\",:]\n",
    "#drop the base column\n",
    "df_percent = df_percent.iloc[1:,:]\n",
    "#transpose and plot\n",
    "df_percent.T.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
