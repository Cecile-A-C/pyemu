{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Model background\n",
    "Here is an example based on the Henry saltwater intrusion problem.  The synthetic model is a 2-dimensional SEAWAT model (X-Z domain) with 1 row, 120 columns and 20 layers.  The left boundary is a specified flux of freshwater, the right boundary is a specified head and concentration saltwater boundary.  The model has two stress periods: an initial steady state (calibration) period, then a transient period with less flux (forecast).  \n",
    "\n",
    "<img src=\"henry/domain.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse problem includes 601 parameters (600 hydraulic conductivity pilot points and 1 specified flux multiplier) and 36 obseravtions (21 heads and 15 concentrations) measured at the end of the steady state calibration period.  Additional, zero-weight observations of head and concentration are also available at each observation location at the end of the transient forecast period as is the distance from the left edge of the domain to the 1%, 10% and 50% saltwater contours in the basal model layer at the end of the forecast stress period.  These distances, named ```pd_one```, ```pd_ten``` and ```pd_half``` are the forecasts we are interested in.  I previously calculated the jacobian matrix, which is in the `henry/` folder, along with the PEST control file.\n",
    "\n",
    "Note we will leave out the 1 specified flux multiplier since it is something that wouldn't typically be estimated.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Using `pyemu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import pyemu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a linear_analysis object.  We will use `schur`  derived type, which replicates the behavior of the `PREDUNC` suite of PEST.  We pass it the name of the jacobian matrix file.  Since we don't pass an explicit argument for `parcov` or `obscov`, `pyemu` attempts to build them from the parameter bounds and observation weights in a pest control file (.pst) with the same base case name as the jacobian.  Since we are interested in forecast uncertainty as well as parameter uncertainty, we also pass the names of the forecast sensitivity vectors we are interested in, which are stored in the jacobian as well.  Note that the `forecasts` argument can be a mixed list of observation names, other jacobian files or PEST-compatible ASCII matrix files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forecasts = [\"pd_one\",\"pd_ten\",\"pd_half\"]\n",
    "la = pyemu.schur(jco=os.path.join(\"henry\", \"pest.jco\"), forecasts=forecasts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The screen output can be redirected to a log file by passing a file name to the `verbose` keyword argument.  Or screen output can be stopped by passing `False` to the `verbose` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "la = pyemu.schur(jco=os.path.join(\"henry\", \"pest.jco\"), forecasts=forecasts,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we drop the 1 specified flux multiplier parameter named ```mult1```.  We ```.get()``` a new ```pyemu``` instance without the ```mult1``` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pnames = la.parcov.row_names\n",
    "pnames.remove(\"mult1\")\n",
    "la = la.get(par_names=pnames,obs_names=la.obscov.row_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the parcov and obscov attributes by saving them to files.  We can save them PEST-compatible ASCII or binary matrices (`.to_ascii()` or `.to_binary()`), PEST-compatible uncertainty files (`.to_uncfile()`), or simply as numpy ASCII arrays (`numpy.savetxt()`).  In fact, all matrix and covariance objects (including the forecasts) have these methods.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "la.parcov.to_uncfile(os.path.join(\"henry\", \"parcov.unc\"), covmat_file=os.path.join(\"henry\",\"parcov.mat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When saving an uncertainty file, if the covariance object is diagonal (`self.isdiagonal == True`), then you can force the uncertainty file to use standard deviation blocks instead of covariance matrix blocks by explicitly passing `covmat_file` as `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "la.obscov.to_uncfile(os.path.join(\"henry\", \"obscov.unc\"), covmat_file=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Posterior parameter uncertainty analysis\n",
    "Let's calculate and save the posterior parameter covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "la.posterior_parameter.to_ascii(os.path.join(\"henry\", \"posterior.mat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open this file in a text editor to examine.  The diagonal of this matrix is the posterior variance of each parameter. Since we already calculated the posterior parameter covariance matrix, additional calls to the `posterior_parameter` decorated method only require access:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "la.posterior_parameter.to_dataframe() #look so nice in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the posterior variance of each parameter along the diagonal of this matrix. Now, let's make a simple plot of prior vs posterior uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = plt.subplot(111)\n",
    "#the prior is already diagonal\n",
    "prior_var = la.parcov.x[:,0]\n",
    "#extract the diagaonl of the posterior\n",
    "post_var = np.diag(la.posterior_parameter.x)\n",
    "#calculate the % uncertainty reduction \n",
    "ureduce = 100.0 * (1.0 - (post_var/prior_var))\n",
    "index = np.arange(600)\n",
    "width = 1.0\n",
    "ax.bar(index,ureduce,width=width,facecolor='b',edgecolor=\"none\")\n",
    "ax.set_ylabel(\"% uncertainty reduction\")\n",
    "ax.set_xlabel(\"parameter number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the at most, the uncertainty of any one of the 600 parameters is only reduced by 7% and the uncertainty of many parameters has not been reduced at all, meaning these parameters are not informed by the observations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Prior forecast uncertainty\n",
    "Now let's examine the prior variance of the forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prior = la.prior_forecast\n",
    "print(prior) # dict keyed on forecast name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it is more intuitive to think in terms of standard deviation, which in this case has units of ```meters``` and can be thought of as the \"+/-\" around the model-predicted distance from the left edge of the domain to the three saltwater concentration contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pname,var in la.prior_forecast.items():\n",
    "    print(pname,np.sqrt(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Posterior forecast uncertainty\n",
    "Now, let's calculate the posterior uncertainty (variance) of each forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post = la.posterior_forecast\n",
    "for pname,var in post.items():\n",
    "    print(pname,np.sqrt(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it - we have completed linear-based uncertainty analysis for a model with 600 parameters and we completed it before actual inversion so we can estimate the worth of continuing and actually completing the expense inversion process!  We can see that the data we have provide atleast some conditioning to each of these forecasts, indicating that the history-matching process is valuable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{0:15s} {1:>10s} {2:>10s} {3:>10s}\".format(\"forecast\",\"prior var\",\"post var\",\"% reduced\"))\n",
    "for pname in prior.keys():\n",
    "    uncert_reduction = 100.0 * ((prior[pname] - post[pname]) / prior[pname])\n",
    "    print(\"{0:15s} {1:10.3f} {2:10.3f} {3:10.3f}\".format(pname,prior[pname],post[pname],uncert_reduction))\n",
    "    #print pname,prior[pname],post[pname],uncert_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting that the uncertainty of the forecasts is reduced substantially even though the uncertainty for any one parameter is only slightly reduced.  This is because the right combinations of forecast-sensitive parameters are being informed by the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data worth\n",
    "Now, let's try to identify which observations are most important to reducing the posterior uncertainty (e.g.the forecast worth of every observation).  In `pyemu`, we define forecast worth as:\n",
    "\n",
    "$100.0 * \\frac{\\text{new variance} - \\text{base variance}}{\\text{new variance}}$\n",
    "\n",
    "where \"base variance\" is the forecast variance using all the observations and \"new variance\" is the forecast variance with some observations left out.\n",
    "\n",
    "First, lets see if the heads or the concentrations are more important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head_names, conc_names = [], []\n",
    "for obs_name in la.jco.obs_names:\n",
    "    if obs_name.startswith('c'):\n",
    "        conc_names.append(obs_name)\n",
    "    elif obs_name.startswith('h'):\n",
    "        head_names.append(obs_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the worth of heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(la.importance_of_observations(head_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the worth of the concentrations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(la.importance_of_observations(conc_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks like the heads and concentrations are both important for reducing the posterior uncertainty of the forecasts.  Let's look at the importance of each individual observation with respect to reducting posterior forecast uncertainty.  Note the observations with an `_2` are actually forecasts but are carried in the pest control file as observations with zero weight. Thats why those observations don't reduce uncertainty.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a structure to hold the results\n",
    "results = {}\n",
    "for name in forecasts:\n",
    "    results[name.lower()] = []\n",
    "# cycle through all of the obs    \n",
    "for oname in la.jco.obs_names:\n",
    "    r = la.importance_of_observations(oname)\n",
    "    #print oname,r\n",
    "    for name,value in r.items():\n",
    "        results[name].append(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a simple plot of the data worth analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = plt.subplot(111)\n",
    "idx = np.arange(len(la.jco.obs_names))\n",
    "width = 0.2\n",
    "colors = ['g', 'b', 'c']\n",
    "for i,name in enumerate(results.keys()):\n",
    "    ax.bar(idx + (i * width), results[name], width=width, edgecolor=\"none\", facecolor=colors[i], label=name)\n",
    "ax.set_xticks(idx + 0.5)\n",
    "ax.set_xticklabels(la.jco.obs_names, rotation=90)\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_ylim(0, ylim[1])\n",
    "ax.set_ylabel(\"% uncertainty reduction\")\n",
    "ax.set_xlabel(\"observation\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that only a handful of observations are conditioning the forecast-sensitive parameters. It appears the each of the head observations is contributing a small amount, but that a single concentration observation (```c_obs15_1``` - concentration location 15) is most important.  This makes sense because only  concentrations measured locations 10 and 15 during the calibration stress period have a non-zero value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
