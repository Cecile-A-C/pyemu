{
 "metadata": {
  "name": "",
  "signature": "sha256:40f40bf5c402082aada7fb64239a1d1350c817ed6e0c3bebc6560be6e9ed4d20"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Model background\n",
      "Here is an example based on the henry saltwater intrusion problem.  The synthetic model is a 2-dimensional SEAWAT model (X-Z domain) with 1 row, 120 columns and 20 layers.  The left boundary is a specified flux of freshwater, the right boundary is a specified head and concentration saltwater boundary.  The model has two stress periods: an initial steady state (calibration) period, then a transient period with less flux (prediction).  \n",
      "\n",
      "![henry domain](domain.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The inverse problem includes 601 parameters (600 hydraulic conductivity pilot points and 1 specified flux rate) and 36 obseravtions (21 heads and 15 concentrations) measured at the end of the steady state calibration period.  Predictions of head and concentration are also available at each observation location at the end of the transient prediction period.  The predictions we will focus on predicting concentrations at location 13, 10, and 05 at the end of the transient stress period.  I previously calculated the jacobian matrix, which is in the `henry/` folder, along with the pest control file.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Using `pyemu`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import os\n",
      "import numpy as np\n",
      "import pyemu\n",
      "reload(pyemu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First create a linear_analysis object.  We will use `schur`  derived type, which replicates the behavior of the `PREDUNC` suite of PEST.  We pass it the name of the jacobian matrix file.  Since we don't pass an explicit argument for `parcov` or `obscov`, `pyemu` attempts to build them from the parameter bounds and observation weights in a pest control file (.pst) with the same base case name as the jacobian.  Since we are interested in predictive uncertainty as well as parameter uncertainty, we also pass the names of the prediction sensitivity vectors we are interested in, which are stored in the jacobian as well.  Note that the `predictions` argument can be a mixed list of observation names, other jacobian files or PEST-compatible ASCII matrix files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictions = [\"C_obs13_2\",\"c_obs10_2\",\"c_obs05_2\"]\n",
      "la = pyemu.schur(jco=os.path.join(\"henry\", \"pest.jco\"), predictions=predictions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The screen output can be redirected to a log file by passing a file name to the `verbose` keyword argument.  Or screen output can be stopped by passing `False` to the `verbose` argument"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "la = pyemu.schur(jco=os.path.join(\"henry\", \"pest.jco\"), predictions=predictions,verbose=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can inspect the parcov and obscov attributes by saving them to files.  We can save them PEST-compatible ASCII or binary matrices (`.to_ascii()` or `.to_binary()`), PEST-compatible uncertainty files (`.to_uncfile()`), or simply as numpy ASCII arrays (`numpy.savetxt()`).  In fact, all matrix and covariance objects (including the predictions) have these methods.  \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "la.parcov.to_uncfile(os.path.join(\"henry\", \"parcov.unc\"), covmat_file=os.path.join(\"henry\",\"parcov.mat\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When saving an uncertainty file, if the covariance object is diagonal (`self.isdiagonal == True`), then you can force the uncertainty file to use standard deviation blocks instead of covariance matrix blocks by explicitly passing `covmat_file` as `None`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "la.obscov.to_uncfile(os.path.join(\"henry\", \"obscov.unc\"), covmat_file=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Posterior parameter uncertainty analysis\n",
      "Let's calculate and save the posterior parameter covariance matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "la.posterior_parameter.to_ascii(os.path.join(\"henry\", \"posterior.mat\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can open this file in a text editor to examine.  The diagonal of this matrix is the posterior variance of each parameter. Since we already calculated the posterior parameter covariance matrix, additional calls to the `posterior_parameter` decorated method only require access:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "la.posterior_parameter.to_dataframe() #look so nice in the notebook"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Prior prediction uncertainty\n",
      "Let's examine the prior variance of the predictions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prior = la.prior_prediction\n",
      "print prior # dict keyed on prediction name\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Posterior prediction uncertainty\n",
      "Now, let's calculate the posterior uncertainty (variance) of each prediction:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post = la.posterior_prediction\n",
      "print post # a dict keyed on prediction name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's it - we have completed linear-based uncertainty analysis for a model with 601 parameters!  We can see that the data we have provide atleast some conditioning to each of these predictions, indicating that the history-matching process is valuable:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"prediction\",\"percent_uncertainty_reduction\"\n",
      "for pname in prior.keys():\n",
      "    uncert_reduction = 100.0 * ((prior[pname] - post[pname]) / prior[pname])\n",
      "    print pname,uncert_reduction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Data worth\n",
      "Now, let's try to identify which observations are most important to reducing the posterior uncertainty (e.g.the predictive worth of every observation).  In `pyemu`, we define predictive worth as $100.0 * \\frac{\\text{new var} - \\text{base var}}{\\text{new var}}$, where \"base var\" is the predictive variance using all the observations and \"new var\" is the predictive variance with some observations left out.\n",
      "\n",
      "First, lets see if the heads or the concentrations are more important.  We'll use `pandas` to get the names of the observations in each group:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "head_names, conc_names = [], []\n",
      "for obs_name in la.jco.obs_names:\n",
      "    if obs_name.startswith('c'):\n",
      "        conc_names.append(obs_name)\n",
      "    elif obs_name.startswith('h'):\n",
      "        head_names.append(obs_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's get the worth of heads:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print la.importance_of_observations(head_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now the worth of the concentrations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print la.importance_of_observations(conc_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, it looks like the heads and concentrations are both important for reducing the posterior uncertainty of the predictions.  Let's look at the importance of each individual observation with respect to reducting posterior predictive uncertainty.  Note the observations with an `_2` are actually predictions but are carried in the pest control file as observations with zero weight. Thats why those observations don't reduce uncertainty.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a structure to hold the results\n",
      "results = {}\n",
      "for pname in predictions:\n",
      "    results[pname.lower()] = []\n",
      "# cycle through all of the obs    \n",
      "for oname in la.jco.obs_names:\n",
      "    r = la.importance_of_observations(oname)\n",
      "    print oname,r\n",
      "    for pname,value in r.iteritems():\n",
      "        results[pname].append(value)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "a simple plot of the data worth analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as plt\n",
      "fig = plt.figure(figsize=(20, 10))\n",
      "ax = plt.subplot(111)\n",
      "idx = np.arange(len(la.jco.obs_names))\n",
      "width = 0.2\n",
      "colors = ['g', 'b', 'c']\n",
      "for i,name in enumerate(results.keys()):\n",
      "    ax.bar(idx + (i * width), results[name], width=width, edgecolor=\"none\", facecolor=colors[i], label=name)\n",
      "ax.set_xticks(idx + 0.5)\n",
      "ax.set_xticklabels(la.jco.obs_names, rotation=90)\n",
      "ylim = ax.get_ylim()\n",
      "ax.set_ylim(0, ylim[1])\n",
      "ax.set_ylabel(\"% uncertainty reduction\")\n",
      "ax.set_xlabel(\"observation\")\n",
      "ax.legend(loc=\"upper left\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we see that only a handful of observations are conditioning the prediction-sensitive parameters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Exploring the link between parameter uncertainty and predictive uncertainty\n",
      "Now lets determine which parameters are contributing most to the uncertainty of the predictions. This takes some time because we have to rerun the analysis with each of the 601 parameter left out, one by one..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a structure for storing the results\n",
      "prior_results, post_results = {}, {}\n",
      "for pname in predictions:\n",
      "    prior_results[pname.lower()] = []\n",
      "    post_results[pname.lower()] = []\n",
      "\n",
      "for iname, pname in enumerate(la.jco.par_names):\n",
      "    r = la.contribution_from_parameters(pname)\n",
      "    print pname, r\n",
      "    for pred,vlist in r.iteritems():\n",
      "        prior_results[pred].append(vlist[0])\n",
      "        post_results[pred].append(vlist[1])\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "a simple figure..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as plt\n",
      "fig = plt.figure(figsize=(20, 10))\n",
      "ax_pr = plt.subplot(211)\n",
      "ax_pt = plt.subplot(212)\n",
      "idx = np.arange(len(la.jco.par_names))\n",
      "width = 0.2\n",
      "colors = ['g', 'b', 'c']\n",
      "for ipred, pred in enumerate(prior_results.keys()):\n",
      "    ax_pr.bar(idx + (width * i),prior_results[pred],edgecolor=\"none\", facecolor=colors[ipred],label=pred)\n",
      "    ax_pt.bar(idx + (width * i),post_results[pred],edgecolor=\"none\", facecolor=colors[ipred])\n",
      "ax_pr.set_title(\"prior\")\n",
      "ax_pt.set_title(\"posterior\")\n",
      "ax_pr.legend(loc=\"upper left\")\n",
      "plt.show() \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This plot doesn't mean much, but you can see some structure in the results: some predictions depend on K properties near surface while other depend on K properties near the bottom of the model domain.  Intuitively, the predictions are not sensitive to K properties near the (farfield) leftside of the domain, only to nearfield, rightside of the domain.  "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}